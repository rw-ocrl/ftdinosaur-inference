{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2632155a-a9cd-4dc9-88a2-81d8b95c331d",
   "metadata": {},
   "source": [
    "# Inference with FT-DINOSAUR"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c11a4ac6-339c-472a-a1fd-99f3b3cf1b93",
   "metadata": {},
   "source": [
    "## Preliminaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95e1668f-36ab-4732-aeeb-e8147f182fc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies when running this on Google Colab\n",
    "try:\n",
    "    import google.colab  # noqa: F401\n",
    "\n",
    "    !pip install \"ftdinosaur-inference[notebook] @ git+https://github.com/ft-dinosaur/ftdinosaur-inference.git\"\n",
    "except ImportError:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c14be83-dd7f-47a4-8496-f98194223cff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "import numpy as np\n",
    "import PIL\n",
    "import torch\n",
    "from IPython.display import display\n",
    "from PIL import Image\n",
    "from torchvision.utils import draw_segmentation_masks\n",
    "\n",
    "from ftdinosaur_inference import build_dinosaur, utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ab67f30-9cb3-4552-8dc5-31a35a8541da",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cmap(num_classes, cmap=\"tab10\"):\n",
    "    cmap = matplotlib.colormaps[cmap].resampled(num_classes)(range(num_classes))\n",
    "    cmap = [tuple((255 * cl[:3]).astype(int)) for cl in cmap]\n",
    "    return cmap\n",
    "\n",
    "\n",
    "def overlay_masks_on_image(\n",
    "    img: PIL.Image, masks: torch.Tensor, num_masks: int, alpha: float = 0.6\n",
    ") -> PIL.Image:\n",
    "    img_tensor = torch.from_numpy(np.array(img)).permute(2, 0, 1)  # C x H x W\n",
    "    height, width = img_tensor.shape[1:]\n",
    "\n",
    "    # Need to resize masks to image (1 x K x P -> 1 x K x H x W)\n",
    "    masks_as_image = utils.resize_patches_to_image(masks, size=(height, width))\n",
    "    masks_as_image = utils.soft_masks_to_one_hot(masks_as_image).squeeze(0)\n",
    "\n",
    "    # Overlay masks on image\n",
    "    masks_on_image = draw_segmentation_masks(\n",
    "        img_tensor, masks_as_image, alpha=alpha, colors=get_cmap(num_masks)\n",
    "    )\n",
    "\n",
    "    # Convert back to PIL\n",
    "    masks_on_image = masks_on_image.permute(1, 2, 0).numpy()\n",
    "    return Image.fromarray((masks_on_image * 255).astype(np.uint8))\n",
    "\n",
    "\n",
    "def load_model():\n",
    "    model = build_dinosaur.build_dinosaur_small_patch14_224_topk3(pretrained=False)\n",
    "    checkpoint_path = \"\"  # TODO\n",
    "    model.load_state_dict(\n",
    "        utils.convert_checkpoint_from_oclf(torch.load(checkpoint_path))\n",
    "    )\n",
    "    model.eval()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb5629a6-ce61-4d0d-9dbf-079adf71a20f",
   "metadata": {},
   "source": [
    "## Define and run model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08140950-63ab-4ac5-96a7-c8375b29e85a",
   "metadata": {},
   "source": [
    "Load the model and define preprocessing. Make sure that `input_size` matches the resolution the model was trained on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21607bb6-682f-40bd-9fd8-65d0c622961a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = load_model()\n",
    "preproc = utils.build_preprocessing(input_size=224)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0699605c-984b-43eb-a00b-ddce3be75cf8",
   "metadata": {},
   "source": [
    "Load an example image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4cbedcc-8a37-4215-9586-9d89d1e1ee20",
   "metadata": {},
   "outputs": [],
   "source": [
    "image = Image.open(\"./images/gizmos.jpg\").convert(\"RGB\")\n",
    "display(image)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fce5c880-17e2-46bb-93ea-5c2c0cadf3e1",
   "metadata": {},
   "source": [
    "Run the model. We can flexibly choose the number of slots using the `num_slots` argument."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "161c5ebb-d8c1-40df-9096-b100421e4c2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    inp = preproc(image).unsqueeze(0)\n",
    "    outp = model(inp, num_slots=6)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18ebb3f8-6ab5-4494-88bc-97fb675fa180",
   "metadata": {},
   "source": [
    "Display the slot masks!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a77fd817-725d-4021-8917-932065dc4553",
   "metadata": {},
   "outputs": [],
   "source": [
    "masks_with_image = overlay_masks_on_image(\n",
    "    image, outp[\"masks\"], num_masks=outp[\"masks\"].shape[1]\n",
    ")\n",
    "display(masks_with_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b83b8a03-fdc8-4506-b666-b6d01c3de046",
   "metadata": {},
   "source": [
    "Note that we used an image with a non-square aspect ratio here. \n",
    "This is achieved by resizing the image to a square in the preprocessing pipeline.\n",
    "The resulting masks are square as well, but are resized to match the original image in `overlay_masks_on_image`.\n",
    "As the model was trained with square images, it generally works better with square images."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

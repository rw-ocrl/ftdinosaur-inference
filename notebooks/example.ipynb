{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2632155a-a9cd-4dc9-88a2-81d8b95c331d",
   "metadata": {},
   "source": [
    "# Inference with FT-DINOSAUR"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c11a4ac6-339c-472a-a1fd-99f3b3cf1b93",
   "metadata": {},
   "source": [
    "## Preliminaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95e1668f-36ab-4732-aeeb-e8147f182fc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies when running this on Google Colab\n",
    "try:\n",
    "    import google.colab  # noqa: F401\n",
    "\n",
    "    !pip install \"ftdinosaur_inference[notebook] @ git+https://github.com/rw-ocrl/ftdinosaur-inference.git\"\n",
    "except ImportError:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c14be83-dd7f-47a4-8496-f98194223cff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "import numpy as np\n",
    "import PIL\n",
    "import torch\n",
    "from IPython.display import display\n",
    "from PIL import Image\n",
    "from torchvision.utils import draw_segmentation_masks\n",
    "\n",
    "from ftdinosaur_inference import build_dinosaur, utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ab67f30-9cb3-4552-8dc5-31a35a8541da",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cmap(num_classes, cmap=\"tab10\"):\n",
    "    cmap = matplotlib.colormaps[cmap].resampled(num_classes)(range(num_classes))\n",
    "    cmap = [tuple((255 * cl[:3]).astype(int)) for cl in cmap]\n",
    "    return cmap\n",
    "\n",
    "\n",
    "def overlay_masks_on_image(\n",
    "    img: PIL.Image, masks: torch.Tensor, num_masks: int, alpha: float = 0.6\n",
    ") -> PIL.Image:\n",
    "    img_tensor = torch.from_numpy(np.array(img)).permute(2, 0, 1)  # C x H x W\n",
    "    height, width = img_tensor.shape[1:]\n",
    "\n",
    "    # Need to resize masks to image (1 x K x P -> 1 x K x H x W)\n",
    "    masks_as_image = utils.resize_patches_to_image(masks, size=(height, width))\n",
    "    masks_as_image = utils.soft_masks_to_one_hot(masks_as_image).squeeze(0)\n",
    "\n",
    "    # Overlay masks on image\n",
    "    masks_on_image = draw_segmentation_masks(\n",
    "        img_tensor, masks_as_image, alpha=alpha, colors=get_cmap(num_masks)\n",
    "    )\n",
    "\n",
    "    # Convert back to PIL\n",
    "    masks_on_image = masks_on_image.permute(1, 2, 0).numpy()\n",
    "    return Image.fromarray(masks_on_image.astype(np.uint8))\n",
    "\n",
    "\n",
    "def load_model(model_name):\n",
    "    model = build_dinosaur.build(model_name, pretrained=True)\n",
    "    model.eval()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb5629a6-ce61-4d0d-9dbf-079adf71a20f",
   "metadata": {},
   "source": [
    "## Define and run model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89040d7c-667e-4f43-bab9-e6c9bda19034",
   "metadata": {},
   "source": [
    "List available models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4b8abd8-3acc-475f-9810-2c9cd41bb3a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "build_dinosaur.list_checkpoints()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08140950-63ab-4ac5-96a7-c8375b29e85a",
   "metadata": {},
   "source": [
    "Load the model and create preprocessing pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21607bb6-682f-40bd-9fd8-65d0c622961a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"dinosaur_base_patch14_518_topk3.coco_dv2_ft_s7_300k+10k\"\n",
    "model = load_model(model_name)\n",
    "preproc = build_dinosaur.build_preprocessing(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0699605c-984b-43eb-a00b-ddce3be75cf8",
   "metadata": {},
   "source": [
    "Load an example image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4cbedcc-8a37-4215-9586-9d89d1e1ee20",
   "metadata": {},
   "outputs": [],
   "source": [
    "image = Image.open(\"./images/example.jpg\").convert(\"RGB\")\n",
    "display(image)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fce5c880-17e2-46bb-93ea-5c2c0cadf3e1",
   "metadata": {},
   "source": [
    "Run the model. We can flexibly choose the number of slots using the `num_slots` argument. Note that the model was trained with 7 slots; thus, picking a number of slots close to 7 works best."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "161c5ebb-d8c1-40df-9096-b100421e4c2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    inp = preproc(image).unsqueeze(0)\n",
    "    outp = model(inp, num_slots=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18ebb3f8-6ab5-4494-88bc-97fb675fa180",
   "metadata": {},
   "source": [
    "Display the slot masks!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a77fd817-725d-4021-8917-932065dc4553",
   "metadata": {},
   "outputs": [],
   "source": [
    "masks_with_image = overlay_masks_on_image(\n",
    "    image, outp[\"masks\"], num_masks=outp[\"masks\"].shape[1]\n",
    ")\n",
    "display(masks_with_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b83b8a03-fdc8-4506-b666-b6d01c3de046",
   "metadata": {},
   "source": [
    "We used an image with a square aspect ratio here, which is what the model was trained with, and what works best in general.\n",
    "Note that the code also supports non-square aspect ratios by resizing the image to a square in the preprocessing pipeline.\n",
    "The resulting masks are square as well, but are resized to match the original image in `overlay_masks_on_image`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a067d03-7b67-4835-b615-16b54c114935",
   "metadata": {},
   "source": [
    "## Upload custom image in Colab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1fccae6-c7f8-4fad-8233-38ab6795cf76",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    from google.colab import files\n",
    "except ImportError as e:\n",
    "    raise ImportError(\"Need to run the following in Google Colab\") from e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f74d907c-5766-4d9b-9ae7-416c9bac22e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Upload image\n",
    "uploaded = files.upload()\n",
    "file_path = list(uploaded.keys())[0]\n",
    "image = Image.open(file_path).convert(\"RGB\")\n",
    "display(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8df8c6dc-c96e-47c0-a38a-252f98070166",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Select number of slots\n",
    "num_slots = 7  # @param {type:\"slider\", min:1, max:24, step:1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "488d3ed1-1af5-421b-8ce2-46da27ed1adb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Run model and display results\n",
    "with torch.no_grad():\n",
    "    inp = preproc(image).unsqueeze(0)\n",
    "    outp = model(inp, num_slots=num_slots)\n",
    "\n",
    "masks_with_image = overlay_masks_on_image(\n",
    "    image, outp[\"masks\"], num_masks=outp[\"masks\"].shape[1]\n",
    ")\n",
    "display(masks_with_image)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
